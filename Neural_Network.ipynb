{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_models:\n",
    "\n",
    "    def create_mlp(Self, dim, regress=False):\n",
    "        # define our MLP network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "        model.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "        # check to see if the regression node should be added\n",
    "        if regress:\n",
    "            model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "        # return our model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(Self, height, width, depth, filters=(16, 32, 64), regress=False):\n",
    "        # initialize the input shape and channel dimension, assuming\n",
    "        # TensorFlow/channels-last ordering\n",
    "\n",
    "        filters = np.asarray(filters)\n",
    "        input_shape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # define the model input\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # loop over the number of filters\n",
    "        for i in range(filters.shape[0]):\n",
    "            # if this is the first CONV layer then set the input\n",
    "            # appropriately\n",
    "            f = filters[i]\n",
    "            if i == 0:\n",
    "                x = inputs\n",
    "\n",
    "            # CONV => RELU => BN => POOL\n",
    "            x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "            x = Activation(\"relu\")(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "            x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "        # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(16)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        # apply another FC layer, this one to match the number of nodes\n",
    "        # coming out of the MLP\n",
    "        x = Dense(4)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "        # check to see if the regression node should be added\n",
    "        if regress:\n",
    "            x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "        # construct the CNN\n",
    "        model = Model(inputs, x)\n",
    "\n",
    "        # return the CNN\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(data, dest):\n",
    "\n",
    "    for i in data.newbookid:\n",
    "        file_path_s = dest + \"\\\\\" + str(i) + '.jpg'\n",
    "        urllib.request.urlretrieve(data.small_image_url[i], file_path_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cover_images(df, input_path):\n",
    "    # initialize our images array\n",
    "    images = []\n",
    "\n",
    "    # loop over the indexes of the books\n",
    "    for i in df.newbookid:\n",
    "\n",
    "        path = input_path + \"\\\\\" + str(i) + '.jpg'\n",
    "        print(path)\n",
    "        image = cv2.imread(path)\n",
    "        outputImage = cv2.resize(image, (32, 32))\n",
    "        images.append(outputImage)\n",
    "\n",
    "    # return our set of images\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_cover_image_data(train_data, test_data, images, scaling):\n",
    "    # process and filter data and label per for the CNN\n",
    "\n",
    "    train_Y = train_data.average_rating\n",
    "    test_Y = test_data.average_rating\n",
    "\n",
    "    train_images = []\n",
    "    for j in train_data.newbookid:\n",
    "        train_images.append(images[j - 1])\n",
    "    test_images = []\n",
    "    for j in test_data.newbookid:\n",
    "        test_images.append(images[j - 1])\n",
    "\n",
    "    train_Y = np.asarray(train_Y) / scaling\n",
    "    test_Y = np.asarray(test_Y) / scaling\n",
    "\n",
    "    return np.asarray(train_images), train_Y, np.asarray(test_images), test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_user_book_data(train_data, test_data, data, scaling):\n",
    "    # process and filter data and label per for the NN\n",
    "\n",
    "    train_Y = train_data.average_rating\n",
    "    test_Y = test_data.average_rating\n",
    "\n",
    "    # Continous data\n",
    "\n",
    "    cont = [\"original_publication_year\", \"pages\"]\n",
    "    cs = MinMaxScaler()\n",
    "\n",
    "    trainCont = cs.fit_transform(train_data[cont])\n",
    "    testCont = cs.transform(test_data[cont])\n",
    "\n",
    "    # Categorical data\n",
    "    categ = [\"first_author\", \"title\", \"firstgenre\"]\n",
    "\n",
    "    for j in range(len(categ)):\n",
    "        bin = LabelBinarizer().fit(data[categ[j]])\n",
    "        if j == 0:\n",
    "            trainFull = np.hstack([trainCont, bin.transform(train_data[categ[j]])])\n",
    "            testFull = np.hstack([testCont, bin.transform(test_data[categ[j]])])\n",
    "        else:\n",
    "            trainFull = np.hstack([trainFull, bin.transform(train_data[categ[j]])])\n",
    "            testFull = np.hstack([testFull, bin.transform(test_data[categ[j]])])\n",
    "\n",
    "    train_Y = np.asarray(train_Y) / scaling\n",
    "    test_Y = np.asarray(test_Y) / scaling\n",
    "\n",
    "    return trainFull, train_Y, testFull, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split, n):\n",
    "    allbooks = random.sample(list(data.newbookid), n)\n",
    "    split_train = np.around(n * split)\n",
    "    train = data[data['newbookid'].isin(allbooks[0:int(split_train) - 1])]\n",
    "    test = data[data['newbookid'].isin(allbooks[int(split_train):n])]\n",
    "\n",
    "    return test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_fit(train_data, train_Y, test_data, test_Y, model):\n",
    "    # train the model\n",
    "    print(\"[INFO] training model...\")\n",
    "    m = model.fit(train_data, train_Y, validation_data=(test_data, test_Y), epochs=25, batch_size=8)\n",
    "\n",
    "    # make predictions on the testing data\n",
    "    print(\"[INFO] predicting book ratings...\")\n",
    "    preds_test = model.predict(test_data)\n",
    "    preds_train = model.predict(train_data)\n",
    "\n",
    "    # compute the difference between the *predicted* book rating and the\n",
    "    # *actual* rating, then compute the percentage difference and\n",
    "    # the absolute percentage difference\n",
    "    diff = preds_test.flatten() - test_Y\n",
    "    percentDiff = (diff / test_Y) * 100\n",
    "    absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "    # compute the mean and standard deviation of the absolute percentage\n",
    "    # difference\n",
    "    mean = np.mean(absPercentDiff)\n",
    "    std = np.std(absPercentDiff)\n",
    "\n",
    "    return preds_test, preds_train, mean, std, m.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r'C:\\Backup\\ayoffe\\Desktop\\Stanford\\Machine Learning\\Project\\goodbooks-10k-master' # define root folder where dsta csv is located\n",
    "\n",
    "if not os.path.isdir(root_path + r'\\NN'):\n",
    "    os.mkdir(root_path + r'\\NN')\n",
    "save_path = root_path + r'\\NN'\n",
    "data = pd.read_csv(root_path + r'\\finalbooks.csv')\n",
    "\n",
    "# Download images from web\n",
    "\n",
    "if not os.path.isdir(root_path + r'\\img_s'):\n",
    "    os.mkdir(root_path + r'\\img_s')\n",
    "input_path = root_path + r'\\img_s'\n",
    "# download_images(data, input_path)\n",
    "\n",
    "data_orig = data\n",
    "data = data.dropna()\n",
    "# Concert pages string to number\n",
    "for i in data.newbookid:\n",
    "    temp = data.pages[data.newbookid == i].astype(str)\n",
    "    temp = temp.str.split(' ')\n",
    "    data.pages[data.newbookid == i] = int(temp.str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "\n",
    "images = load_cover_images(data_orig, input_path)\n",
    "images = images / 255.0\n",
    "\n",
    "NN_type = np.asarray([1, 2, 3])  # 1 - CNN based on cover images, 2 - NN based on dataset, 3 - combined NN\n",
    "scaling = 5  # rating normalization\n",
    "\n",
    "n_train = np.asarray([20, 1200, 500, 1000, 2000, 4000, data.shape[0]])\n",
    "\n",
    "RMS = np.zeros((n_train.shape[0], 4))  # cols: number of books; image based RMSE, data based RMSE, mix based RMSE\n",
    "c = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in n_train:\n",
    "    c += 1\n",
    "    RMS[c, 0] = u\n",
    "\n",
    "    test_data, train_data = split_data(data, 0.75, u)\n",
    "\n",
    "    for n_type in NN_type:\n",
    "        preds = []\n",
    "        loss_vec = []\n",
    "        train_vec = []\n",
    "        # create model\n",
    "        model = NN_models()\n",
    "\n",
    "        if n_type == 1:\n",
    "            model_CNN = model.create_cnn(32, 32, 3, regress=True)\n",
    "            opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "            model_CNN.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "            [train_images, train_Y, test_images, test_Y] = Create_cover_image_data(train_data, test_data, images,\n",
    "                                                                                   scaling)\n",
    "            [preds_test, preds_train, mean, std, loss] = NN_fit(train_images, train_Y, test_images, test_Y, model_CNN)\n",
    "            np.savetxt(save_path + r'\\Preds_Test_CNN_' + str(u) + '.csv', preds_test)\n",
    "            np.savetxt(save_path + r'\\Preds_Train_CNN_' + str(u) + '.csv', preds_train)\n",
    "            np.savetxt(save_path + r'\\Loss_CNN_' + str(u) + '.csv', loss)\n",
    "            rmse_test = np.sqrt(np.mean((preds_test * scaling - np.asarray(test_Y.reshape(-1, 1)) * scaling) ** 2))\n",
    "            rmse_train = np.sqrt(np.mean((preds_train * scaling - np.asarray(train_Y.reshape(-1, 1)) * scaling) ** 2))\n",
    "            with open(save_path + r'\\RMSE_Test_CNN_' + str(u) + '.txt', 'w') as f:\n",
    "                f.write('%f' % rmse_test)\n",
    "            RMS[c, n_type] = rmse_test\n",
    "            with open(save_path + r'\\RMSE_Train_CNN_' + str(u) + '.txt', 'w') as f:\n",
    "                f.write('%f' % rmse_train)\n",
    "            RMS[c, n_type] = rmse_test\n",
    "        elif n_type == 2:\n",
    "            [trainFull, train_Y, testFull, test_Y] = Create_user_book_data(train_data, test_data, data,\n",
    "                                                                           scaling)\n",
    "            model_NN = model.create_mlp(trainFull.shape[1], regress=True)\n",
    "            opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "            model_NN.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "            [preds_test, preds_train, mean, std, loss] = NN_fit(trainFull, train_Y, testFull, test_Y, model_NN)\n",
    "            np.savetxt(save_path + r'\\Preds_test_MLP_NN_' + str(u) + '.csv', preds_test)\n",
    "            np.savetxt(save_path + r'\\Preds_train_MLP_NN_' + str(u) + '.csv', preds_train)\n",
    "            np.savetxt(save_path + r'\\Loss_MLP_NN_' + str(u) + '.csv', loss)\n",
    "            rmse_test = np.sqrt(np.mean((preds_test * scaling - np.asarray(test_Y.reshape(-1, 1)) * scaling) ** 2))\n",
    "            rmse_train = np.sqrt(np.mean((preds_train * scaling - np.asarray(train_Y.reshape(-1, 1)) * scaling) ** 2))\n",
    "            with open(save_path + r'\\RMSE_Test_MLP_NN_' + str(u) + '.txt', 'w') as f:\n",
    "                f.write('%f' % rmse_test)\n",
    "            RMS[c, n_type] = rmse_test\n",
    "            with open(save_path + r'\\RMSE_Train_MLP_NN_' + str(u) + '.txt', 'w') as f:\n",
    "                f.write('%f' % rmse_train)\n",
    "            RMS[c, n_type] = rmse_test\n",
    "        else:\n",
    "\n",
    "            # create the MLP and CNN models\n",
    "\n",
    "            mlp = model.create_mlp(trainFull.shape[1], regress=False)\n",
    "            cnn = model.create_cnn(32, 32, 3, regress=False)\n",
    "\n",
    "            # create the input to our final set of layers as the *output* of both\n",
    "            # the MLP and CNN\n",
    "            combinedInput = concatenate([mlp.output, cnn.output])\n",
    "            # our final FC layer head will have two dense layers, the final one\n",
    "            # being our regression head\n",
    "            x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "            x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "            # our final model will accept categorical/numerical data on the MLP\n",
    "            # input and images on the CNN input, outputting a single value (the\n",
    "            # predicted price of the house)\n",
    "            model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "            opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "            model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "\n",
    "            # train the model\n",
    "            print(\"[INFO] training model...\")\n",
    "            m = model.fit(\n",
    "                [trainFull, train_images], train_Y,\n",
    "                validation_data=([testFull, test_images], test_Y),\n",
    "                epochs=25, batch_size=8)\n",
    "\n",
    "            # make predictions on the testing data\n",
    "            print(\"[INFO] predicting book ratings...\")\n",
    "            preds_test = model.predict([testFull, test_images])\n",
    "            preds_train = model.predict([trainFull, train_images])\n",
    "\n",
    "            diff = preds_test.flatten() - test_Y\n",
    "            percentDiff = (diff / test_Y) * 100\n",
    "            absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "            # compute the mean and standard deviation of the absolute percentage\n",
    "            # difference\n",
    "            mean = np.mean(absPercentDiff)\n",
    "            std = np.std(absPercentDiff)\n",
    "            loss = m.history['val_loss']\n",
    "\n",
    "            np.savetxt(save_path + r'\\Preds_test_Mix_' + str(u) + '.csv', preds_test)\n",
    "            np.savetxt(save_path + r'\\Preds_train_Mix_' + str(u) + '.csv', preds_train)\n",
    "            np.savetxt(save_path + r'\\Loss_Mix_' + str(u) + '.csv', loss)\n",
    "            rmse_test = np.sqrt(np.mean((preds_test * scaling - np.asarray(test_Y.reshape(-1, 1)) * scaling) ** 2))\n",
    "            rmse_train = np.sqrt(np.mean((preds_train * scaling - np.asarray(train_Y.reshape(-1, 1)) * scaling) ** 2))\n",
    "            with open(save_path + r'\\RMSE_Test_MIX_' + str(u) + '.txt', 'w') as f:\n",
    "                f.write('%f' % rmse_test)\n",
    "            RMS[c, n_type] = rmse_test\n",
    "            with open(save_path + r'\\RMSE_Train_MIX_' + str(u) + '.txt', 'w') as f:\n",
    "                f.write('%f' % rmse_train)\n",
    "            RMS[c, n_type] = rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullRMSE = pd.DataFrame(\n",
    "    {'n_users': RMS[:, 0], 'Image_based': RMS[:, 1], 'Data_based': RMS[:, 2], 'Mix_Model': RMS[:, 3]})\n",
    "fullRMSE.to_csv(save_path + r'\\All_RMSE.csv', index=False)\n",
    "\n",
    "plt.figure()\n",
    "fmt_CNN = '[o][-][b]'\n",
    "fmt_MLP = '[s][-][r]'\n",
    "fmt_Mix = '[*][-][g]'\n",
    "plt.plot(RMS[:, 0] * 0.75, RMS[:, 1], 'bo-', label='Cover image based CNN', linewidth=1.5)\n",
    "plt.plot(RMS[:, 0] * 0.75, RMS[:, 2], 'gs-', label='DataBase based MPL', linewidth=1.5)\n",
    "plt.plot(RMS[:, 0] * 0.75, RMS[:, 3], 'r^-', label='Mixed Model', linewidth=1.5)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add labels and save to disk\n",
    "plt.title('RMSE as function of Data size')\n",
    "plt.xlabel('# of Books for Train set')\n",
    "plt.ylabel('RMSE on test set')\n",
    "plt.legend()\n",
    "plt.savefig(save_path + r'\\RMSE_vs_users.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
