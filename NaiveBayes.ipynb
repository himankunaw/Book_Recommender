{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalbooks = pd.read_csv('finalbook.csv')\n",
    "ratings = pd.read_csv('finalratings.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_k(r,k):\n",
    "    '''Discounted Cumulative Gain(DCG)\n",
    "    r: True Ratings in Predicted Rank Order(1st element is top recommendation)\n",
    "    k: Number of results to consider \n",
    "    '''\n",
    "    \n",
    "    r = np.asfarray(r)[:k]\n",
    "    dcg = np.sum(2**r / np.log2(np.arange(2, r.size + 2)))\n",
    "    return dcg\n",
    "\n",
    "def ndcg_k(r,k):\n",
    "    \"Normalized Discounted Cumulative Gain(NDCG)\"\n",
    "    \n",
    "    dcg_max = dcg_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return dcg_k(r,k) / dcg_max\n",
    "\n",
    "def mean_ndcg(rs):\n",
    "    '''Mean NDCG for all users\n",
    "    rs: Iterator/For each user: True ratings in Predicted Rank orde\n",
    "    '''\n",
    "    \n",
    "    mean = np.mean([ndcg_k(r, len(r)) for r in rs])\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,h):\n",
    "    '''Root Mean Squared Error(RMSE)\n",
    "    y: real y\n",
    "    h: predicted y\n",
    "    '''\n",
    "    \n",
    "    a=y-h\n",
    "    return np.sqrt(sum(a**2)/len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the tail\n",
    "tailcomp = ratings.groupby(by='newbookid', as_index=False).agg({'rating': pd.Series.count}).sort_values(by='rating', ascending=False)\n",
    "tot=sum(tailcomp['rating'])\n",
    "tailcomp['popshare'] = [x/tot for x in tailcomp['rating']]\n",
    "tailcomp['popshare'] = tailcomp['popshare'].cumsum()\n",
    "tailcomp['category'] = ['Head' if x<0.95 else \"Tail\" for x in tailcomp['popshare']]\n",
    "tail = tailcomp.loc[tailcomp.popshare>=0.95]\n",
    "tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(message):\n",
    "    '''Get the normalized list of words from a message string.\n",
    "    This function should split a message into words, normalize them and return the resulting list.\n",
    "    For splitting, you should split on spaces. For normalization, you should convert everything to lowercase.\n",
    "    '''\n",
    "    \n",
    "    words = message\n",
    "    words = message.split(\" \")\n",
    "    words = [x.lower() for x in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(messages):\n",
    "    '''Create a dictionary mapping words to integer in dices\n",
    "    '''\n",
    "    \n",
    "    word_counts = collections.defaultdict(int)\n",
    "    \n",
    "    for message in messages:\n",
    "        for word in set(get_words(message)):\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    resulting_dictionary={}\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count>=25 and word not in stopwords.words('english') and len(word) >1:\n",
    "            next_index = len(resulting_dictionary)\n",
    "            resulting_dictionary[word] = next_index\n",
    "    \n",
    "    return resulting_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(messages, word_dictionary):\n",
    "    \"Transform a list of text messages into a numpy array for further processing.\"\n",
    "    \n",
    "    A = np.zeros((len(messages), len(word_dictionary)))\n",
    "    \n",
    "    for i, message in enumerate(messages):\n",
    "        for word in get_words(message):\n",
    "            if word in word_dictionary:\n",
    "                A[i, word_dictionary[word]] +=1\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes_model(matrix, labels):\n",
    "    \"Fit a naive bayes model.\"\n",
    "    \n",
    "    model = {}\n",
    "\n",
    "    phi = (1. * sum(labels) / len(labels))*0.95+0.05*0.5\n",
    "    model['logphi_0'] = np.log(1.-phi)\n",
    "    model['logphi_1'] = np.log(phi)\n",
    "    theta_0 = (matrix[labels == 0]).sum(axis=0) + 1\n",
    "    theta_1 = (matrix[labels == 1]).sum(axis=0) + 1\n",
    "    theta_0 /= theta_0.sum()\n",
    "    theta_1 /= theta_1.sum()\n",
    "    model['logtheta_0'] = np.log(theta_0)\n",
    "    model['logtheta_1'] = np.log(theta_1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_naive_bayes_model(model, matrix):\n",
    "    \"Use a Naive Bayes model to compute predictions for a target matrix.\"\n",
    "\n",
    "    output = np.zeros(matrix.shape[0])\n",
    "\n",
    "    logphi_0 = model['logphi_0']\n",
    "    logphi_1 = model['logphi_1']\n",
    "    logtheta_0 = model['logtheta_0']\n",
    "    logtheta_1 = model['logtheta_1']\n",
    "    logprobs_0 = (matrix * logtheta_0).sum(axis=1) + logphi_0\n",
    "    logprobs_1 = (matrix * logtheta_1).sum(axis=1) + logphi_1\n",
    "\n",
    "    output = (logprobs_1/(logprobs_1+logprobs_0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_five_naive_bayes_words(model, dictionary):\n",
    "   \n",
    "    ids = np.argsort(model['logtheta_0'] - model['logtheta_1'])[:5]\n",
    "\n",
    "    reverse_dictionary = {i: word for word, i in dictionary.items()}\n",
    "\n",
    "    return [reverse_dictionary[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "finalbooks['book_desc'] = finalbooks['book_desc'].fillna(finalbooks['title'])\n",
    "finalbooks['book_desc'] = finalbooks['book_desc'].str.replace(r'[^\\w\\s]',\"\")\n",
    "finalbooks['book_desc'] = finalbooks['book_desc'].fillna(finalbooks['tag_cloud'])\n",
    "finalbooks['tag_cloud'] = finalbooks['tag_cloud'].str.replace('-',\" \")\n",
    "finalbooks['words'] = finalbooks['book_desc'] +\" \"+finalbooks['tag_cloud']+\" \"+finalbooks['authors']\n",
    "dico = create_dictionary(finalbooks['book_desc'])\n",
    "len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = transform_text(finalbooks['book_desc'], dico)\n",
    "finalbooks['binary']= [1 if x >=4 else 0 for x in finalbooks['average_rating']]\n",
    "ratings['binary']= [1 if x >=4 else 0 for x in ratings['rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = []\n",
    "topwords = []\n",
    "indicators = np.zeros(len(dico))\n",
    "for i in range(15000):\n",
    "    User = train.loc[train.newuser_id == i+1].sort_values('newbookid')\n",
    "    User['binary']= [1 if x >=4 else 0 for x in User['rating']]\n",
    "    A[User['newbookid']-1,:] \n",
    "    model = fit_naive_bayes_model(A[User['newbookid']-1,:], User['binary'])\n",
    "    result = predict_from_naive_bayes_model(model, A)\n",
    "    UserRes = finalbooks.filter(['newbookid'])\n",
    "    UserRes['newuser_id'] = i+1 \n",
    "    UserRes['pred'] = result\n",
    "    allpreds.append(UserRes)\n",
    "    indicators = indicators + (model['logtheta_0'] - model['logtheta_1'])\n",
    "    if (i+1)%1000 == 0:\n",
    "        print(\"done: \", i+1)\n",
    "## Append in a list and then use concat\n",
    "top5 = get_top_five_naive_bayes_words(model, dico)\n",
    "topwords.append(top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = indicators*15000\n",
    "ids = np.argsort(-indicators)[:5]\n",
    "reverse_dictionary = {i: word for word, i in dico.items()}\n",
    "[reverse_dictionary[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(indicators*1000000000000)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivewords = np.concatenate(topwords, axis=0 )\n",
    "from collections import Counter\n",
    "for key, value in sorted(Counter(fivewords).items(), reverse=True, key=lambda item: item[1]):\n",
    "    print(\"%s: %s\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(allpreds, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes =pd.DataFrame(predictions, columns=['newbookid','newuser_id', 'pred']) \n",
    "bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesrank = test.merge(bayes,on = ['newbookid', 'newuser_id'])\n",
    "bayesrank = bayesrank.sort_values(by=['newuser_id', 'pred'], ascending=False)\n",
    "bayesrank.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesrank['pred']=bayesrank['pred']*4+1\n",
    "bayesrank.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesrank['pred'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['conc']=train['newuser_id'].map(str)+train['newbookid'].map(str)\n",
    "bayes['conc']=bayes['newuser_id'].map(str)+bayes['newbookid'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesfin = bayes[~bayes.conc.isin(train.conc)]\n",
    "bayesfin.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayeslist = []\n",
    "for i in range(15000):\n",
    "    a = bayesrank.loc[bayesrank.newuser_id == i+1]['rating'].tolist()\n",
    "    bayeslist.append(a)\n",
    "    if (i+1)%1000 == 0:\n",
    "        print(\"done: \", i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([ndcg_k(r, len(r)) for r in bayeslist])\n",
    "\n",
    "facet, axes = plt.subplots(1, 1, figsize=(10, 3))\n",
    "n, bins, patches = plt.hist(b, 200, facecolor='blue', alpha=0.5) #, log = True)   \n",
    "plt.title('Distribution of NDGC among Users for the Bayes model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b[b == 1]\n",
    "sum(d)/15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(1) Bayes Model RMSE: ', np.round(rmse(bayesrank['pred'],bayesrank['rating']), decimals=3))\n",
    "print('(2) Bayes Model NDCG: ', np.round(mean_ndcg(bayeslist), decimals=3))\n",
    "print(\"(3) Median NDCG: \", np.round(np.median(b), decimals=3))\n",
    "print(\"(4) Share of NDCG =1 among Users: \", np.round(sum(d)/15000, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesranktrain = train.merge(bayes,on = ['newbookid', 'newuser_id'])\n",
    "bayesranktrain = bayesranktrain.sort_values(by=['newuser_id', 'pred'], ascending=False)\n",
    "bayesranktrain['pred']=bayesranktrain['pred']*4+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayeslisttrain = []\n",
    "for i in range(15000):\n",
    "    a = bayesranktrain.loc[bayesranktrain.newuser_id == i+1]['rating'].tolist()\n",
    "    bayeslisttrain.append(a)\n",
    "    if (i+1)%1000 == 0: print(\"done: \", i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(1) Bayes Model Train RMSE: ', np.round(rmse(bayesranktrain['pred'],bayesranktrain['rating']), decimals=3))\n",
    "print('(2) Bayes Model Train NDCG: ', np.round(mean_ndcg(bayeslisttrain), decimals=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
